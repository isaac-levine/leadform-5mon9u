# AI Service Kubernetes Manifest
# Version: 1.0.0
# Purpose: Deploy and configure AI service with high availability and monitoring

# Service definition for AI service
apiVersion: v1
kind: Service
metadata:
  name: ai-service
  namespace: app
  labels:
    app: ai-service
    component: backend
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
spec:
  ports:
    - port: 8000
      targetPort: http
      protocol: TCP
      name: http
    - port: 8080
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app: ai-service
  type: ClusterIP

---
# Horizontal Pod Autoscaler for dynamic scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-service
  namespace: app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

---
# Pod Disruption Budget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-service-pdb
  namespace: app
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ai-service

---
# Main Deployment configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-service
  namespace: app
  labels:
    app: ai-service
    component: backend
    version: "{{ .Values.image.tag }}"
  annotations:
    kubernetes.io/change-cause: "{{ .Values.deployment.changeCause }}"
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ai-service
  template:
    metadata:
      labels:
        app: ai-service
        version: "{{ .Values.image.tag }}"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
        linkerd.io/inject: "enabled"
    spec:
      # Pod distribution and scheduling
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - ai-service
                topologyKey: kubernetes.io/hostname
      
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      
      # Container configuration
      containers:
        - name: ai-service
          image: "{{ .Values.image.repository }}/ai-service:{{ .Values.image.tag }}"
          imagePullPolicy: Always
          
          # Container ports
          ports:
            - containerPort: 8000
              protocol: TCP
              name: http
            - containerPort: 8080
              protocol: TCP
              name: metrics
          
          # Environment configuration
          env:
            - name: APP_NAME
              value: "ai-service"
            - name: ENV
              value: "production"
            - name: PORT
              value: "8000"
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-service-secrets
                  key: openai-api-key
            - name: REDIS_HOST
              value: "redis-master"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-password
                  key: redis-password
            - name: MODEL_CACHE_SIZE
              value: "512"
            - name: MAX_CONCURRENT_REQUESTS
              value: "100"
          
          # Resource limits and requests
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "1000m"
              memory: "1Gi"
          
          # Health checks
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          startupProbe:
            httpGet:
              path: /health/startup
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 30